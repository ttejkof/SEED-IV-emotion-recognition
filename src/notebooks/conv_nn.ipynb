{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 15, 24, 10)\n",
      "(3, 15, 24, 10, 4, 62, 10)\n",
      "(3, 15, 24, 10)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all_features_4sec_epoch.npy\")\n",
    "\n",
    "# data = data[:, :, :, 2:6]\n",
    "\n",
    "labels_ids = [[1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3],\n",
    "        [2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1],\n",
    "        [1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]]\n",
    "print(len(labels_ids))\n",
    "labels = np.zeros(data.shape[:-3])\n",
    "for session in range(data.shape[0]):\n",
    "    for video in range(data.shape[2]):\n",
    "        labels[session,:,video] = labels_ids[session][video]\n",
    "\n",
    "print(labels.shape) # (3, 15, 24, 8)\n",
    "print(data.shape) # (3, 15, 24, 8, 4, 62, 10)\n",
    "print(np.logical_or((labels == 0), (labels == 2)).shape) # (3, 15, 24, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(features, labels, test_subjects=[0,1,2]):\n",
    "    test_data = features[:, test_subjects]\n",
    "    test_labels = labels[:, test_subjects]\n",
    "\n",
    "    train_data = np.delete(features, test_subjects, axis=1)\n",
    "    train_labels = np.delete(labels, test_subjects, axis=1)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accs = []\n",
    "for i in range(15):\n",
    "    train_data, train_labels, test_data, test_labels = train_test(data, labels, test_subjects=[i])\n",
    "\n",
    "    train_mask = np.logical_or((train_labels == 0), (train_labels == 2))\n",
    "    test_mask = np.logical_or((test_labels == 0), (test_labels == 2))\n",
    "\n",
    "    train_labels = train_labels[train_mask]\n",
    "    train_data = train_data[train_mask]\n",
    "\n",
    "    test_labels = test_labels[test_mask]\n",
    "    test_data = test_data[test_mask]\n",
    "\n",
    "    # test_data = np.reshape(test_data, (test_data.shape[0], -1))\n",
    "    # train_data = np.reshape(train_data, (train_data.shape[0], -1))\n",
    "\n",
    "    train_labels[train_labels == 0] = 0\n",
    "    test_labels[test_labels == 0] = 0\n",
    "\n",
    "    test_labels[test_labels == 2] = 1\n",
    "    train_labels[train_labels == 2] = 1\n",
    "\n",
    "    # print(train_data.shape)\n",
    "    # print(train_labels.shape)\n",
    "\n",
    "    # print(test_data.shape)\n",
    "    # print(test_labels.shape)\n",
    "    # knn = KNeighborsClassifier(n_neighbors=100)\n",
    "    # knn.fit(train_data,train_labels)\n",
    "    # predicted_data = knn.predict(test_data)\n",
    "    # acc = accuracy_score(test_labels,predicted_data)\n",
    "    # print(f\"i {i}, acc {acc}\")\n",
    "    # accs.append(acc)\n",
    "print(np.mean(accs))\n",
    "# normalize data\n",
    "# train_data = (train_data - np.mean(train_data, axis=0)) / np.std(train_data, axis=0)\n",
    "# test_data = (test_data - np.mean(train_data, axis=0)) / np.std(train_data, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = train_test(data, labels, test_subjects=[i])\n",
    "\n",
    "train_mask = np.logical_or((train_labels == 0), (train_labels == 2))\n",
    "test_mask = np.logical_or((test_labels == 0), (test_labels == 2))\n",
    "\n",
    "train_labels = train_labels[train_mask]\n",
    "train_data = train_data[train_mask]\n",
    "\n",
    "test_labels = test_labels[test_mask]\n",
    "test_data = test_data[test_mask]\n",
    "\n",
    "# test_data = np.reshape(test_data, (sveska.ipynb.shape[0], -1))\n",
    "\n",
    "train_labels[train_labels == 0] = 0\n",
    "test_labels[test_labels == 0] = 0\n",
    "\n",
    "test_labels[test_labels == 2] = 1\n",
    "train_labels[train_labels == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KORELACIONA ANALIZA SA LABELAMA\n",
    "# df1 = pd.DataFrame(train_data)\n",
    "# df2 = pd.DataFrame(test_data)\n",
    "# df = pd.concat([df1, df2])\n",
    "# df1['label'] = np.concatenate([train_labels])\n",
    "# df2['label'] = np.concatenate([test_labels])\n",
    "\n",
    "# df['label'] = np.concatenate((train_labels, test_labels))\n",
    "# # save df to csv\n",
    "# df.to_csv('data.csv', index=False)\n",
    "\n",
    "# df2.to_csv('test_data.csv', index=False)\n",
    "# df1.to_csv('train_data.csv', index=False)\n",
    "\n",
    "# df\n",
    "# corr_analysis=abs(df.corr()['label']).sort_values(ascending=False)\n",
    "# corr_analysis = corr_analysis[corr_analysis.index!='label']\n",
    "# corr_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (5040, 4, 62, 10), (5040,), (360, 4, 62, 10), (360,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes: {train_data.shape}, {train_labels.shape}, {test_data.shape}, {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_channels=4, num_features_1=62, num_features_2=10, conv1_out_channels=16, conv2_out_channels=32, conv3_out_channels=64, fc1_out_features=128, fc2_out_features=64):\n",
    "        super(EEGEmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=conv1_out_channels, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_out_channels, out_channels=conv2_out_channels, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32)\n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_out_channels, out_channels=conv3_out_channels, kernel_size=(3, 3), stride=1, padding=1, dtype=torch.float32)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1920, fc1_out_features)\n",
    "        self.fc2 = nn.Linear(fc1_out_features, fc2_out_features)\n",
    "        self.fc3 = nn.Linear(fc2_out_features, 1)  # Output layer for binary classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU and pooling\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        # Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 15, 24, 10)\n",
      "(3, 15, 24, 10, 4, 62, 10)\n",
      "(3, 15, 24, 10)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all_features_4sec_epoch.npy\")\n",
    "\n",
    "# data = data[:, :, :, 2:6]\n",
    "\n",
    "labels_ids = [[1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3],\n",
    "        [2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1],\n",
    "        [1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]]\n",
    "print(len(labels_ids))\n",
    "labels = np.zeros(data.shape[:-3])\n",
    "for session in range(data.shape[0]):\n",
    "    for video in range(data.shape[2]):\n",
    "        labels[session,:,video] = labels_ids[session][video]\n",
    "\n",
    "print(labels.shape) # (3, 15, 24, 8)\n",
    "print(data.shape) # (3, 15, 24, 8, 4, 62, 10)\n",
    "print(np.logical_or((labels == 0), (labels == 2)).shape) # (3, 15, 24, 8)\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = train_test(data, labels, test_subjects=[i])\n",
    "\n",
    "train_mask = np.logical_or((train_labels == 0), (train_labels == 2))\n",
    "test_mask = np.logical_or((test_labels == 0), (test_labels == 2))\n",
    "\n",
    "train_labels = train_labels[train_mask]\n",
    "train_data = train_data[train_mask]\n",
    "\n",
    "test_labels = test_labels[test_mask]\n",
    "test_data = test_data[test_mask]\n",
    "\n",
    "\n",
    "train_labels[train_labels == 0] = 0\n",
    "test_labels[test_labels == 0] = 0\n",
    "\n",
    "test_labels[test_labels == 2] = 1\n",
    "train_labels[train_labels == 2] = 1\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "hidden_size = 500\n",
    "net = EEGEmotionClassifier(num_channels=4, num_features_1=62, num_features_2=10, conv1_out_channels=16, conv2_out_channels=32, conv3_out_channels=64, fc1_out_features=128, fc2_out_features=64).to('cuda')\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=0.0001)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Normalize train and test data\n",
    "\n",
    "\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32).to('cuda')\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long).to('cuda')\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDatasets for training and testing data\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoaders for training and testing data\n",
    "train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "# train_labels = F.one_hot(train_labels, num_classes=4).float()\n",
    "# test_labels = F.one_hot(test_labels, num_classes=4).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15762/467034894.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.6961286067962646 train_accuracy 0.5005952380952381 test_accuracy 0.5\n",
      "New best val accuracy: 0.5 at epoch 0\n",
      "Epoch 10, loss 0.7102961540222168 train_accuracy 0.5 test_accuracy 0.5\n",
      "Epoch 20, loss 0.6924713253974915 train_accuracy 0.5027777777777778 test_accuracy 0.5055555555555555\n",
      "New best val accuracy: 0.5055555555555555 at epoch 20\n",
      "Epoch 30, loss 0.6900306344032288 train_accuracy 0.4998015873015873 test_accuracy 0.5305555555555556\n",
      "New best val accuracy: 0.5305555555555556 at epoch 30\n",
      "Epoch 40, loss 0.6875377893447876 train_accuracy 0.5077380952380952 test_accuracy 0.5694444444444444\n",
      "New best val accuracy: 0.5694444444444444 at epoch 40\n",
      "Epoch 50, loss 0.6836801171302795 train_accuracy 0.5027777777777778 test_accuracy 0.5722222222222222\n",
      "New best val accuracy: 0.5722222222222222 at epoch 50\n",
      "Epoch 60, loss 0.6798761487007141 train_accuracy 0.49563492063492065 test_accuracy 0.6166666666666667\n",
      "New best val accuracy: 0.6166666666666667 at epoch 60\n",
      "Epoch 70, loss 0.6796131730079651 train_accuracy 0.4976190476190476 test_accuracy 0.5694444444444444\n",
      "Epoch 80, loss 0.6892743706703186 train_accuracy 0.5047619047619047 test_accuracy 0.5166666666666667\n",
      "Epoch 90, loss 0.6916028261184692 train_accuracy 0.4936507936507937 test_accuracy 0.5277777777777778\n",
      "Epoch 100, loss 0.6792001128196716 train_accuracy 0.4998015873015873 test_accuracy 0.5472222222222223\n",
      "Epoch 110, loss 0.6795083284378052 train_accuracy 0.48928571428571427 test_accuracy 0.5805555555555556\n",
      "Epoch 120, loss 0.67774498462677 train_accuracy 0.5041666666666667 test_accuracy 0.5805555555555556\n",
      "Epoch 130, loss 0.7007227540016174 train_accuracy 0.49563492063492065 test_accuracy 0.5166666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 27\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(out)\n\u001b[1;32m     30\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(train_labels\u001b[38;5;241m.\u001b[39mcpu(), out \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "net.apply(init_weights)\n",
    "model = net\n",
    "best_val_acc = 0\n",
    "for epoch in range(100000):\n",
    "    net.train()\n",
    "    # clear cuda cache\n",
    "    torch.cuda.empty_cache()\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = net(data)[..., 0]\n",
    "\n",
    "        loss = loss_fn(out.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        out = []\n",
    "        \n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            out.append(net(data)[..., 0].cpu())\n",
    "        out = torch.cat(out)\n",
    "\n",
    "        acc = accuracy_score(train_labels.cpu(), out > 0.5)\n",
    "        # acc = accuracy_score(torch.argmax(train_labels, dim=1).cpu(), torch.argmax(out, dim=1).cpu())\n",
    "        # test the model\n",
    "        with torch.no_grad():\n",
    "            outs = []\n",
    "            for i, (data, labels) in enumerate(test_loader):\n",
    "                data = data.to('cuda')\n",
    "                out = net(data)[..., 0]\n",
    "                loss = loss_fn(out.cpu(), labels.float())\n",
    "                outs.append(out.cpu())\n",
    "            out = torch.cat(outs)\n",
    "            test_acc = accuracy_score(test_labels.cpu(), out > 0.5)\n",
    "        print(f'Epoch {epoch}, loss {loss.item()} train_accuracy {acc} test_accuracy {test_acc}')\n",
    "\n",
    "        if test_acc > best_val_acc:\n",
    "            best_val_acc = test_acc\n",
    "            print(f'New best val accuracy: {best_val_acc} at epoch {epoch}')    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate confusion matrix\n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     out = net(test_data.to('cuda'))\n",
    "#     predicted_labels = torch.argmax(out, dim=1)\n",
    "#     test_non_one_hot = torch.argmax(test_labels, dim=1)\n",
    "#     test_acc = accuracy_score(test_non_one_hot, predicted_labels.cpu())\n",
    "#     cm = confusion_matrix(test_non_one_hot, predicted_labels.cpu())\n",
    "#     print(f\"Test accuracy: {test_acc}\")\n",
    "#     print(cm)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0','1','2','3'])\n",
    "#     disp.plot()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 100, 2000)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    \n",
    "    # model = nn.Sequential(\n",
    "    #     nn.Linear(10*62*4, hidden_size*2),\n",
    "    #     nn.BatchNorm1d(hidden_size*2),\n",
    "    #     nn.LeakyReLU(),\n",
    "    #     nn.Dropout(dropout_rate),\n",
    "    #     nn.Linear(hidden_size*2, hidden_size),\n",
    "    #     nn.BatchNorm1d(hidden_size),\n",
    "    #     nn.LeakyReLU(),\n",
    "    #     nn.Dropout(dropout_rate),\n",
    "    #     nn.Linear(hidden_size, hidden_size//2),\n",
    "    #     nn.BatchNorm1d(hidden_size // 2),\n",
    "    #     nn.LeakyReLU(),\n",
    "    #     nn.Dropout(dropout_rate),\n",
    "    #     nn.Linear(hidden_size//2, 1),\n",
    "    #     nn.Sigmoid()\n",
    "    # ).to('cuda')\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(10*62*4, hidden_size),\n",
    "        nn.BatchNorm1d(hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "        nn.Sigmoid()\n",
    "    ).to('cuda')\n",
    "    model.apply(init_weights)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model, optimizer = create_model(trial)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(500):    \n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Training loop with DataLoader\n",
    "        # for batch_data, batch_labels in train_loader:\n",
    "        #     batch_data, batch_labels = batch_data.to('cuda'), batch_labels.to('cuda')\n",
    "        #     optimizer.zero_grad()\n",
    "        #     out = model(batch_data)\n",
    "        #     loss = loss_fn(out[..., 0], batch_labels.float())\n",
    "        #     optimizer.step()\n",
    "            \n",
    "        #     loss.backward()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data)\n",
    "        # print(f\"out: {out.shape}, train_labels: {train_labels.shape}, train_data: {train_data.shape}\")\n",
    "        loss = loss_fn(out[..., 0], train_labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Calculate training accuracy\n",
    "            # all_train_out = []\n",
    "            # for batch_data, _ in train_loader:\n",
    "            #     batch_data = batch_data.to('cuda')\n",
    "            #     out = model(batch_data)\n",
    "            #     all_train_out.append(out[..., 0].cpu())\n",
    "            # all_train_out = torch.cat(all_train_out)\n",
    "            # train_acc = accuracy_score(train_labels.cpu(), all_train_out > 0.5)\n",
    "            \n",
    "            # # Calculate validation accuracy\n",
    "            # all_test_out = []\n",
    "            # for batch_data, _ in test_loader:\n",
    "            #     batch_data = batch_data.to('cuda')\n",
    "            #     out = net(batch_data)\n",
    "            #     all_test_out.append(out[..., 0].cpu())\n",
    "            # all_test_out = torch.cat(all_test_out)\n",
    "            # val_acc = accuracy_score(test_labels, all_test_out > 0.5)\n",
    "            \n",
    "            out = out[..., 0].detach().cpu() > 0.5\n",
    "            out = out\n",
    "            train_acc = accuracy_score(train_labels.cpu(), out)\n",
    "            # acc = accuracy_score(torch.argmax(train_labels, dim=1).cpu(), torch.argmax(out, dim=1).cpu())\n",
    "            # test the model\n",
    "            with torch.no_grad():\n",
    "                out = net(test_data.to('cuda'))\n",
    "                # predicted_labels = torch.argmax(out, dim=1)\n",
    "                # test_non_one_hot = torch.argmax(test_labels, dim=1)\n",
    "                val_acc = accuracy_score(test_labels, out[..., 0].cpu() > 0.5)\n",
    "\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            print(f'New best val accuracy: {best_val_acc} at epoch {epoch}')\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch}, loss {loss.item()}, train_acc {train_acc}, val_accuracy {val_acc}')\n",
    "    \n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 17:52:11,360] A new study created in memory with name: no-name-3cebeba2-b258-4cd1-a2e6-d30e84b1b15a\n",
      "/tmp/ipykernel_13991/1746426871.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best val accuracy: 0.5222222222222223 at epoch 0\n",
      "Epoch 0, loss 0.8216129541397095, train_acc 0.5001984126984127, val_accuracy 0.5222222222222223\n",
      "New best val accuracy: 0.5388888888888889 at epoch 1\n",
      "New best val accuracy: 0.5444444444444444 at epoch 7\n",
      "New best val accuracy: 0.55 at epoch 22\n",
      "New best val accuracy: 0.5583333333333333 at epoch 40\n",
      "New best val accuracy: 0.5611111111111111 at epoch 46\n",
      "Epoch 50, loss 0.8271164894104004, train_acc 0.4996031746031746, val_accuracy 0.5333333333333333\n",
      "New best val accuracy: 0.5638888888888889 at epoch 66\n",
      "Epoch 100, loss 0.8359272480010986, train_acc 0.49404761904761907, val_accuracy 0.5361111111111111\n",
      "Epoch 150, loss 0.835940420627594, train_acc 0.5007936507936508, val_accuracy 0.5333333333333333\n",
      "Epoch 200, loss 0.8274425864219666, train_acc 0.5063492063492063, val_accuracy 0.5361111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-03 17:52:21,781] Trial 0 failed with parameters: {'hidden_size': 1344, 'dropout_rate': 0.40195477741862107, 'lr': 0.017016176881559823, 'weight_decay': 4.692128365995673e-06} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/drakula/.local/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13991/4244170460.py\", line 49, in objective\n",
      "    out = out[..., 0].detach().cpu() > 0.5\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-03 17:52:21,782] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[43], line 49\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# all_train_out = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# all_test_out = torch.cat(all_test_out)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# val_acc = accuracy_score(test_labels, all_test_out > 0.5)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     50\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     51\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m accuracy_score(train_labels\u001b[38;5;241m.\u001b[39mcpu(), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=1)\n",
    "\n",
    "\n",
    "print(f'Best hyperparameters: {study.best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
